{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "395a5c9e-9e2e-45de-a178-994acec66ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#perceptron implementation without activation function and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "99729e80-15e7-46b5-abd1-346a2c2af530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "057d4e70-0672-4e1f-8141-3e9d442d08cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wSum(X,W): #X is mostly numpy array \n",
    "    h = torch.from_numpy(X)\n",
    "    z = torch.matmul(W,h)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7f6c07fb-f872-403d-8a36-1230d758ab19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activate (x): #sigmoid\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4cfe930d-e1d2-4050-8194-ae2ef64b701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_step(X,W_list): #X is mostly numpy array \n",
    "    h = torch.from_numpy(X)\n",
    "    for W in W_list:\n",
    "        z = torch.matmul(W,h)\n",
    "        h=activate(z)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "777b6f9e-f3f6-4be6-a06e-02c6ecad1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateParams(W_list,dW_list,lr): \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(W_list)):\n",
    "            W_list[i] -= lr*dW_list[i]\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a9b080de-4cec-4d0b-bf09-4fede18172fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_batch(X,y,W_list,loss_fn,lr=0.0001, nepochs =100):\n",
    "    n = len(y)\n",
    "    for epoch in range(nepochs):\n",
    "        loss = 0\n",
    "        for i in range(len(y)):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forward_step(Xin, W_list)\n",
    "            loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "        loss = loss/n\n",
    "        loss.backward()\n",
    "        sys.stdout.flush()\n",
    "        dW_list=[]\n",
    "        for k in range(len(W_list)):\n",
    "            dW_list.append(W_list[k].grad.data)\n",
    "        W_list = updateParams(W_list, dW_list,lr)\n",
    "        for j in range(len(W_list)):\n",
    "            W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch = %d: %f\" %(epoch, loss))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "58b87751-5fc1-40f7-b965-8e2d7f3848bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_sgd(X,y,W_list,loss_fn,lr=0.0001, nepochs =100):\n",
    "    for epoch in range(nepochs):\n",
    "        avgLoss = []\n",
    "        for i in range(len(y)):\n",
    "            Xin = X[i,:]\n",
    "            yTrue = y[i]\n",
    "            y_hat = forward_step(Xin, W_list)\n",
    "            loss = loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss.backward()\n",
    "            avgLoss.append(loss.item())\n",
    "            sys.stdout.flush()\n",
    "            dW_list=[]\n",
    "            for k in range(len(W_list)):\n",
    "                dW_list.append(W_list[k].grad.data)\n",
    "            W_list = updateParams(W_list, dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch = %d: %f\" %(epoch, np.mean(np.array(avgLoss))))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f283c73a-8193-41c8-98f2-01a98d629382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001, nepochs =100,batchSize=16):\n",
    "    n = len(y)\n",
    "    numBatches= n//batchSize;\n",
    "    \n",
    "    for epoch in range(nepochs):\n",
    "        for batch in range(numBatches):\n",
    "            X_batch = X[batch*batchSize:(batch+1)*batchSize,:]\n",
    "            y_batch = y[batch*batchSize:(batch+1)*batchSize]    \n",
    "            loss = 0\n",
    "            for i in range(batchSize):\n",
    "                Xin = X_batch[i,:]\n",
    "                yTrue = y_batch[i]\n",
    "                y_hat = forward_step(Xin, W_list)\n",
    "                loss += loss_fn(y_hat,torch.tensor(yTrue,dtype=torch.double))\n",
    "            loss = loss/batchSize\n",
    "            loss.backward()\n",
    "            sys.stdout.flush()\n",
    "            dW_list=[]\n",
    "            for k in range(len(W_list)):\n",
    "                dW_list.append(W_list[k].grad.data)\n",
    "            W_list = updateParams(W_list, dW_list,lr)\n",
    "            for j in range(len(W_list)):\n",
    "                W_list[j].grad.data.zero_()\n",
    "        print(\"Loss after epoch = %d: %f\" %(epoch, loss/numBatches))\n",
    "    return W_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "96f462b3-4692-492c-adef-7316a1ea2f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch = 0: 0.014618\n",
      "Loss after epoch = 1: 0.014602\n",
      "Loss after epoch = 2: 0.014586\n",
      "Loss after epoch = 3: 0.014571\n",
      "Loss after epoch = 4: 0.014555\n",
      "Loss after epoch = 5: 0.014540\n",
      "Loss after epoch = 6: 0.014525\n",
      "Loss after epoch = 7: 0.014509\n",
      "Loss after epoch = 8: 0.014494\n",
      "Loss after epoch = 9: 0.014479\n",
      "Loss after epoch = 10: 0.014464\n",
      "Loss after epoch = 11: 0.014449\n",
      "Loss after epoch = 12: 0.014434\n",
      "Loss after epoch = 13: 0.014419\n",
      "Loss after epoch = 14: 0.014404\n",
      "Loss after epoch = 15: 0.014389\n",
      "Loss after epoch = 16: 0.014374\n",
      "Loss after epoch = 17: 0.014359\n",
      "Loss after epoch = 18: 0.014345\n",
      "Loss after epoch = 19: 0.014330\n",
      "Loss after epoch = 20: 0.014315\n",
      "Loss after epoch = 21: 0.014301\n",
      "Loss after epoch = 22: 0.014286\n",
      "Loss after epoch = 23: 0.014272\n",
      "Loss after epoch = 24: 0.014257\n",
      "Loss after epoch = 25: 0.014243\n",
      "Loss after epoch = 26: 0.014229\n",
      "Loss after epoch = 27: 0.014214\n",
      "Loss after epoch = 28: 0.014200\n",
      "Loss after epoch = 29: 0.014186\n",
      "Loss after epoch = 30: 0.014172\n",
      "Loss after epoch = 31: 0.014158\n",
      "Loss after epoch = 32: 0.014144\n",
      "Loss after epoch = 33: 0.014130\n",
      "Loss after epoch = 34: 0.014116\n",
      "Loss after epoch = 35: 0.014102\n",
      "Loss after epoch = 36: 0.014088\n",
      "Loss after epoch = 37: 0.014075\n",
      "Loss after epoch = 38: 0.014061\n",
      "Loss after epoch = 39: 0.014047\n",
      "Loss after epoch = 40: 0.014034\n",
      "Loss after epoch = 41: 0.014020\n",
      "Loss after epoch = 42: 0.014007\n",
      "Loss after epoch = 43: 0.013993\n",
      "Loss after epoch = 44: 0.013980\n",
      "Loss after epoch = 45: 0.013967\n",
      "Loss after epoch = 46: 0.013953\n",
      "Loss after epoch = 47: 0.013940\n",
      "Loss after epoch = 48: 0.013927\n",
      "Loss after epoch = 49: 0.013914\n",
      "Loss after epoch = 50: 0.013901\n",
      "Loss after epoch = 51: 0.013888\n",
      "Loss after epoch = 52: 0.013875\n",
      "Loss after epoch = 53: 0.013862\n",
      "Loss after epoch = 54: 0.013849\n",
      "Loss after epoch = 55: 0.013836\n",
      "Loss after epoch = 56: 0.013823\n",
      "Loss after epoch = 57: 0.013810\n",
      "Loss after epoch = 58: 0.013798\n",
      "Loss after epoch = 59: 0.013785\n",
      "Loss after epoch = 60: 0.013772\n",
      "Loss after epoch = 61: 0.013760\n",
      "Loss after epoch = 62: 0.013747\n",
      "Loss after epoch = 63: 0.013735\n",
      "Loss after epoch = 64: 0.013722\n",
      "Loss after epoch = 65: 0.013710\n",
      "Loss after epoch = 66: 0.013698\n",
      "Loss after epoch = 67: 0.013685\n",
      "Loss after epoch = 68: 0.013673\n",
      "Loss after epoch = 69: 0.013661\n",
      "Loss after epoch = 70: 0.013649\n",
      "Loss after epoch = 71: 0.013637\n",
      "Loss after epoch = 72: 0.013624\n",
      "Loss after epoch = 73: 0.013612\n",
      "Loss after epoch = 74: 0.013600\n",
      "Loss after epoch = 75: 0.013589\n",
      "Loss after epoch = 76: 0.013577\n",
      "Loss after epoch = 77: 0.013565\n",
      "Loss after epoch = 78: 0.013553\n",
      "Loss after epoch = 79: 0.013541\n",
      "Loss after epoch = 80: 0.013530\n",
      "Loss after epoch = 81: 0.013518\n",
      "Loss after epoch = 82: 0.013506\n",
      "Loss after epoch = 83: 0.013495\n",
      "Loss after epoch = 84: 0.013483\n",
      "Loss after epoch = 85: 0.013472\n",
      "Loss after epoch = 86: 0.013460\n",
      "Loss after epoch = 87: 0.013449\n",
      "Loss after epoch = 88: 0.013438\n",
      "Loss after epoch = 89: 0.013426\n",
      "Loss after epoch = 90: 0.013415\n",
      "Loss after epoch = 91: 0.013404\n",
      "Loss after epoch = 92: 0.013393\n",
      "Loss after epoch = 93: 0.013382\n",
      "Loss after epoch = 94: 0.013370\n",
      "Loss after epoch = 95: 0.013359\n",
      "Loss after epoch = 96: 0.013348\n",
      "Loss after epoch = 97: 0.013337\n",
      "Loss after epoch = 98: 0.013327\n",
      "Loss after epoch = 99: 0.013316\n"
     ]
    }
   ],
   "source": [
    "inputDim = 10\n",
    "n = 1000 #datapoint number\n",
    "X = np.random.rand(n, inputDim)\n",
    "y = np.random.randint(0,2,n) #classification problem with two classes\n",
    "\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True) #all weights for cmputational layer 1\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True) #all weights for cmputational layer 2\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True) #all weights for cmputational layer 3\n",
    "\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "\n",
    "loss_fn = nn.BCELoss()\n",
    "#W_list = trainNN_sgd(X,y,W_list,loss_fn, lr = 0.0001,nepochs=100)\n",
    "#W_list = trainNN_batch(X,y,W_list,loss_fn,lr=0.0001, nepochs =100)\n",
    "W_list = trainNN_minibatch(X,y,W_list,loss_fn,lr=0.0001, nepochs =100,batchSize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feecbcf-3d40-4ec1-b26f-329fb5fecbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7664bf42-571d-4b68-960a-68fa56128b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000 #datapoint number\n",
    "X = np.random.rand(n, inputDim)\n",
    "y = np.random.randint(0,2,n) #classification problem with two classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89b3e7d-2ce3-409d-9988-818447bde360",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2271bd40-a217-4cf7-9852-e02f51b6bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d216f14-826a-41b2-88b4-d3053be7ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5fb744-8c77-4d3b-a208-c52590381c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.random.uniform(0,1,inputDim),requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58f9428-eca4-424e-8e1a-c32b4ab9d636",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = wSum(X[0,:],W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c40570-e95e-48d9-b739-41f25dfe9bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c90cf4-7fdb-400b-b357-f8d0d7c323c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 10\n",
    "n = 1000 #datapoint number\n",
    "X = np.random.rand(n, inputDim)\n",
    "y = np.random.randint(0,2,n) #classification problem with two classes\n",
    "W1 = torch.tensor(np.random.uniform(0,1,(2,inputDim)),requires_grad=True) #all weights for cmputational layer 1\n",
    "W2 = torch.tensor(np.random.uniform(0,1,(3,2)),requires_grad=True) #all weights for cmputational layer 2\n",
    "W3 = torch.tensor(np.random.uniform(0,1,3),requires_grad=True) #all weights for cmputational layer 3\n",
    "W_list = []\n",
    "W_list.append(W1)\n",
    "W_list.append(W2)\n",
    "W_list.append(W3)\n",
    "z= forward_step(X[0,:],W_list)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71698fa6-9ba4-4b52-963d-f2381630e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fb82e3-3b3e-4e25-bba1-29f8ad95952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fun1= nn.Sigmoid()\n",
    "activation_fun2 = nn.ReLU()\n",
    "x = 100*torch.randn(1)\n",
    "print(x,activation_fun1(x), activation_fun2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69549b5-fab0-4080-ab72-6eba1e048076",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_fun= nn.Sigmoid()\n",
    "x = torch.randn(1)\n",
    "y = torch.randint(0,2,(1,),dtype=torch.float) #target\n",
    "y_hat = activation_fun(x)\n",
    "\n",
    "#loss function\n",
    "loss_fun = nn.BCELoss()\n",
    "loss_val = loss_fun(y_hat,y)\n",
    "\n",
    "print(loss_val.item())\n",
    "print(y_hat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26fda82-1826-4bb7-ba40-93a08ca0f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7140d827-eff9-4d63-be26-ff799bb44cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Sigmoid()\n",
    "loss_fun = nn.BCELoss()\n",
    "lr = 0.0001\n",
    "x = torch.randn(1)\n",
    "y= torch.randint(0,2,(1,),dtype=torch.float)\n",
    "w = torch.randn(1,requires_grad = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7d763-490a-432c-8179-c722bf971bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "nIter = 100\n",
    "for i in range(nIter):\n",
    "    y_hat = m(w*x)\n",
    "    loss = loss_fun(y_hat,y)\n",
    "    loss.backward()\n",
    "    dw = w.grad.data\n",
    "    with torch.no_grad():\n",
    "        w -=lr*dw\n",
    "    w.grad.data.zero_()\n",
    "    print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b5ab4f-92e9-4a09-b7ac-9791791aba11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
